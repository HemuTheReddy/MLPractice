# -*- coding: utf-8 -*-
"""05-transferLearning-FineTuning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17RtTZ9Gsi4Q14hJRqokpI4LT9o1xHib5

# Transfer Learning with TensorFlow - Fine Tuning
"""

# Check if we're using gpu

!nvidia-smi

"""## Creating helper functions

We could rewrite them, but this is tedious
"""

!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py

#Import helper functions we're going to use in this notebook

from helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, walk_through_dir

"""## Let's get some data

This time, we're going to see how we can use the pretrained model within tf.keras.applications and apply them to our own problem (recogniziing images of food)


"""

!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip

unzip_data("10_food_classes_10_percent.zip")

# Check out how many images and sub directories our in our data set

walk_through_dir("10_food_classes_10_percent")

# Create training and test directory paths

train_dir = "10_food_classes_10_percent/train/"
test_dir = "10_food_classes_10_percent/test/"

import tensorflow as tf

IMG_SIZE = (224,224)
BATCH_SIZE=32
train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=train_dir,
                                                                            image_size=IMG_SIZE,
                                                                            label_mode="categorical",
                                                                            batch_size=BATCH_SIZE)
test_data = tf.keras.preprocessing.image_dataset_from_directory(directory=test_dir,
                                                                image_size=IMG_SIZE,
                                                                label_mode="categorical",
                                                                batch_size=BATCH_SIZE)

train_data_10_percent

# Check out the class names of our dataset
train_data_10_percent.class_names

# See an example of a batch of data
for images, labels in train_data_10_percent.take(1):
  print(images, labels)

"""## Model 0: Building a transfer learning model using the Keras Sequential API

The sequential API is straightforward, it runs out layers in sequential order, but the functional api gives us more flexibility with our models

"""

# 1. Create the base model tf.keras.applications
base_model = tf.keras.applications.EfficientNetB0(include_top=False)

# 2. Freeze the base model (so the underlying pretrained patterns aren't updated during training)
base_model.trainable = False

# 3. Create inputs into our model
inputs = tf.keras.layers.Input(shape=(224,224, 3), name = "Input Layer")

# 4. If using ResNet50V2 you will need to normalize inputs
#x = tf.keras.layers.experimental.preprocessing.Rescaling(1/255.)(inputs)

# 5. Pass the inputs to the base_model
x = base_model(inputs)

print(f"Shape after passing inputs through base model: {x.shape}")

# 6. Average pool the outputs of the base model (aggregate all the most import information, reduce number of computations)
x = tf.keras.layers.GlobalAveragePooling2D(name = "global_average_pooling_layer")(x)
print(f"Shape after global average pooling 2d: {x.shape}")

# 7. Create the output activation layer
outputs = tf.keras.layers.Dense(10, activation ="softmax", name = "output_layer")(x)

# 8. Combine the inputs with the outputs into a model
model_0 = tf.keras.Model(inputs, outputs)

# 9. Compile the model

model_0.compile(loss="categorical_crossentropy",
                optimizer=tf.keras.optimizers.Adam(),
                metrics=["accuracy"])

model_0_history = model_0.fit(train_data_10_percent,
                              epochs = 5,
                              steps_per_epoch = len(train_data_10_percent),
                              validation_data = train_data_10_percent,
                              validation_steps = int(len(test_data)*.25),
                              callbacks = [create_tensorboard_callback("transfer_learning",
                                                                       "10_percent_feature_extraction",)])

#Evaluate on the full test dataset
model_0.evaluate(test_data)

# Check the layers in our base model
for layer_number, layer in enumerate(base_model.layers):
  print(layer_number, layer.name)

# Summary of base model
base_model.summary()

# How about a summary of our whole model?
model_0.summary()

# Check out our model's training curves

plot_loss_curves(model_0_history)

"""## Getting a feature vector from a trained model

Let's demonstrate the global average pooling 2d layer


"""

# Define the input shape
input_shape = (1,4,4,3)

# create random tensor
tf.random.set_seed(42)
input_tensor = tf.random.normal(input_shape)
print(f"random input tensor \n {input_tensor} \n")

# Pass the random tensor through a global average pooling 2d layer
global_average_pooled_tensor = tf.keras.layers.GlobalAveragePooling2D()(input_tensor)
print(f"2D global average pooled random tensor \n {global_average_pooled_tensor} \n")

# Check shape
print(f"Shape of input tensor: {input_tensor.shape}")
print(f"Shape of input global average pooled 2d tensor: {global_average_pooled_tensor.shape}")

# Let's replicate the Global Average Pooled 2D Layer
tf.reduce_mean(input_tensor, axis =[1,2])

"""## Running a series of transfer learning experiments

We've seen the incredible results transfer learning can get with only 10% of the training data, but how does it go with 1% of the training data... how about we setup a bunch of experiments to find out
"""

# Download and unzip data
!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_1_percent.zip

unzip_data("10_food_classes_1_percent.zip")

train_dir = "10_food_classes_1_percent/train/"
test_dir = "10_food_classes_1_percent/test/"

# How many images are we working with
walk_through_dir("10_food_classes_1_percent")

# Setup data loaders
IMG_SIZE = (224,224)
train_data_1_percent = tf.keras.preprocessing.image_dataset_from_directory(train_dir,
                                                                           label_mode="categorical",
                                                                           image_size=IMG_SIZE,
                                                                           batch_size=BATCH_SIZE)
test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir,
                                                                label_mode="categorical",
                                                                image_size=IMG_SIZE,
                                                                batch_size=BATCH_SIZE)

"""## Adding data augmentation right into the model


"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers.experimental import preprocessing

# Create data augmentation stage with horizontal flipping, rotation, zooms, etc.
data_augmentation = keras.Sequential([
    preprocessing.RandomFlip("horizontal"),
    preprocessing.RandomRotation(0.2),
    preprocessing.RandomZoom(0.2),
    preprocessing.RandomHeight(0.2),
    preprocessing.RandomWidth(0.2),
    #preprocessing.Rescale(1/255.)
], name = "data_augmentation")

"""## Visualize data ugmentation layer (and see what happens to our data)"""

# View a random image and compare it to its augmented version

import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import os
import random
target_class = random.choice(train_data_1_percent.class_names)
target_dir = "10_food_classes_1_percent/train/" + target_class
random_image = random.choice(os.listdir(target_dir))
random_image_path = target_dir + "/" + random_image

# Read in random image
img = mpimg.imread(random_image_path)

plt.imshow(img)
plt.title(f"Original Random image from {target_class}")
plt.axis(False)

# Now let's plot our augmented random image
augmented_image = data_augmentation(tf.expand_dims(img, axis = 0))
plt.figure()
plt.imshow(tf.squeeze(augmented_image)/255.)
plt.title(f"Augmented random image from class {target_class}")
plt.axis(False)

"""## Model 1: FE transfer learning on1% w/ data augmentation"""

# Setup input shape and base model

input_shape = (224,224,3)
base_model = tf.keras.applications.EfficientNetB0(include_top=False)
base_model.trainable=False

# Create input layer
inputs = layers.Input(shape=input_shape, name="input_layer")
#Data augmentation sequential model as a layer
x = data_augmentation(inputs)

#give base model inputs after augmented
x = base_model(x, training=False)

# Pool output features of base model
x = layers.GlobalAveragePooling2D(name="global_average_pooling_layer")(x)

# put a dense layer on as the ouput
outputs = layers.Dense(10, activation="softmax", name="output_layer")(x)

model_1 = keras.Model(inputs, outputs)

model_1.compile(loss="categorical_crossentropy",
               optimizer=tf.keras.optimizers.Adam(),
               metrics=["accuracy"])
history_1 = model_1.fit(train_data_1_percent,
                        epochs =5,
                        steps_per_epoch = len(train_data_1_percent),
                        validation_data = test_data,
                        validation_steps = int(0.25*len(test_data)),
                        callbacks = [create_tensorboard_callback("transfer_learning",
                                                                "1_percent_data_aug")])

# model summary
model_1.summary()

# Evaluate on full test dataset
model_1.evaluate(test_data)

# How do the model with 1% of the data and data aug loss curves look

plot_loss_curves(history_1)

# Setup input shape and base model
input_shape = (224,224,3)
base_model = keras.applications.EfficientNetB0(include_top=False)
base_model.trainable=False

inputs = layers.Input(shape=input_shape, name="input_layer")
x = data_augmentation(inputs)
x = base_model(x, training = False)
x = layers.GlobalAveragePooling2D(name="Global_average_pooled_layer")(x)
outputs = layers.Dense(10, activation="softmax", name="output_layer")(x)

model_2 = keras.Model(inputs, outputs)

model_2.compile(loss="categorical_crossentropy",
               optimizer=tf.keras.optimizers.Adam(),
               metrics="accuracy")

"""## Create a modelcheckpoint callback"""

# Set checkpoint path
checkpoint_path = "ten_percent_model_checkpoints_weights/checkpoint.ckps"

# Create a model checkpoint callback that saves the model's weights only
checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                         save_weights_only=True,
                                                         save_best_only=False,
                                                         save_freq="epoch",
                                                         verbose=1)

history_2 = model_2.fit(train_data_10_percent,
                        epochs=5,
                        steps_per_epoch=len(train_data_10_percent),
                        validation_data=test_data,
                        validation_steps=int(.25*len(test_data)),
                        callbacks=[create_tensorboard_callback(dir_name="transfer_learning",
                                                              experiment_name="10_percent_data_aug"),
                                  checkpoint_callback])

plot_loss_curves(history_2)

"""### Loading in checkpointed weights

### Loading in checkpointed weights

-returns a model to a specific checkpoint
"""

# Load in saved model weights and evaluate model
model_2.load_weights(checkpoint_path)

